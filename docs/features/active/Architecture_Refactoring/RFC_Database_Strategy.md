# RFC: Database Storage Strategy (ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì „ëµ)

> **ë²„ì „**: 1.0.0
> **ì‘ì„±ì¼**: 2026-01-07
> **ìƒíƒœ**: Draft
> **ëŒ€ìƒ**: ì‹¤ì‹œê°„ ì•ˆí…Œë‚˜ ì¶”ì  ë°ì´í„° DB ì €ì¥

## ê°œìš”

### ëª©ì 
ì‹¤ì‹œê°„ UDP ìˆ˜ì‹  ë°ì´í„°(ì´ˆë‹¹ 100ê°œ)ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ DBì— ì €ì¥í•˜ì—¬:
- âœ… ì‹¤ì‹œê°„ ì„±ëŠ¥ ë°©í•´ ì—†ìŒ (ë¹„ë™ê¸°)
- âœ… ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥ (timestamp ë³´ì¡´)
- âœ… ì¥ì•  ë³µì›ë ¥ (í ë²„í¼ë§)
- âœ… ì¡°íšŒ ì„±ëŠ¥ ìµœì í™” (ì¸ë±ìŠ¤, íŒŒí‹°ì…”ë‹)

### ë¬¸ì„œ ë²”ìœ„

> âš ï¸ **í˜„ì¬ ë‹¨ê³„ ì•ˆë‚´**
>
> ì´ ë¬¸ì„œëŠ” **ì‹¤ì‹œê°„ ì¶”ì  ë°ì´í„°(tracking_data) ì €ì¥ ì „ëµ**ë§Œ ë‹¤ë£¹ë‹ˆë‹¤.
>
> **ì „ì²´ DB ìŠ¤í‚¤ë§ˆ ì„¤ê³„ëŠ” DB ë³¸ê²© ë„ì… ì‹œ ë³„ë„ ì§„í–‰**ë©ë‹ˆë‹¤:
> - ìœ„ì„± ì •ë³´(satellites), íŒ¨ìŠ¤ ìŠ¤ì¼€ì¤„(pass_schedules), ì„¤ì •(configurations) ë“±
> - ERD(Entity-Relationship Diagram) ì‘ì„±
> - í…Œì´ë¸” ê´€ê³„ ë° ì°¸ì¡° ë¬´ê²°ì„± ì •ì˜
> - ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
>
> í˜„ì¬ëŠ” **ì‹¤ì‹œê°„ ë°ì´í„° ì €ì¥ ì•„í‚¤í…ì²˜ ê²€ì¦**ì— ì§‘ì¤‘í•©ë‹ˆë‹¤.

### í•µì‹¬ ì „ëµ
**Event Time ê¸°ë°˜ ë¹„ë™ê¸° ë°°ì¹˜ ì €ì¥**

```
UDP (10ms) â†’ ë©”ëª¨ë¦¬ (ì¦‰ì‹œ) â†’ WebSocket (30ms) â†’ Frontend
              â†“
            í ë²„í¼ (ë¹„ë™ê¸°)
              â†“
          1ì´ˆë§ˆë‹¤ 100ê°œ ë°°ì¹˜ ì €ì¥
              â†“
            DB (PostgreSQL)
```

---

## 1. ì•„í‚¤í…ì²˜ ì„¤ê³„

### 1.1 ë°ì´í„° íë¦„

```kotlin
// Event Time ìº¡ì²˜
fun onUdpReceived(packet: UdpPacket) {
    val eventTime = Instant.now()  // â† ì‹¤ì œ ë°ì´í„° ì‹œê°„

    val data = TrackingData(
        timestamp = eventTime,      // Event Time (ì¤‘ìš”!)
        azimuth = packet.azimuth,
        elevation = packet.elevation,
        train = packet.train,
        satelliteId = packet.satelliteId
    )

    // ì‹¤ì‹œê°„ ê²½ë¡œ (í”„ë¡ íŠ¸ì—”ë“œìš©)
    dataStoreService.update(data)

    // ì €ì¥ ê²½ë¡œ (DBìš©, ë¹„ë™ê¸°)
    queue.offer(data)
}
```

### 1.2 ë¹„ë™ê¸° ë°°ì¹˜ ì €ì¥

```kotlin
@Service
class AsyncDatabaseWriter(
    private val repository: TrackingDataRepository
) {
    // ìµœëŒ€ 10,000ê°œ ë²„í¼ (100ì´ˆ ë¶„ëŸ‰)
    private val queue = LinkedBlockingQueue<TrackingData>(10_000)
    private val saveRate = AtomicLong(0)

    init {
        // ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤
        thread(isDaemon = true, name = "db-writer") {
            val batch = mutableListOf<TrackingData>()

            while (true) {
                try {
                    // 1ì´ˆ ë™ì•ˆ ë°ì´í„° ìˆ˜ì§‘
                    val deadline = System.currentTimeMillis() + 1000

                    while (System.currentTimeMillis() < deadline && batch.size < 100) {
                        queue.poll(100, TimeUnit.MILLISECONDS)?.let {
                            batch.add(it)
                        }
                    }

                    // ë°°ì¹˜ ì €ì¥
                    if (batch.isNotEmpty()) {
                        repository.batchInsert(batch)
                        saveRate.addAndGet(batch.size.toLong())
                        logger.debug("ì €ì¥: ${batch.size}ê°œ, ë²”ìœ„: ${batch.first().timestamp}~${batch.last().timestamp}")
                        batch.clear()
                    }

                } catch (e: Exception) {
                    logger.error("DB ì €ì¥ ì‹¤íŒ¨", e)
                    // ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰ (íì— ë°ì´í„° ìœ ì§€)
                }
            }
        }
    }

    fun add(data: TrackingData): Boolean = queue.offer(data)

    fun getPendingCount(): Int = queue.size

    fun getSaveRate(): Long = saveRate.getAndSet(0)
}
```

---

## 2. ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ

### 2.1 ë©”ì¸ í…Œì´ë¸” (íŒŒí‹°ì…”ë‹)

```sql
-- PostgreSQL: ì›”ë³„ íŒŒí‹°ì…˜
CREATE TABLE tracking_data (
    id BIGSERIAL,
    timestamp TIMESTAMP NOT NULL,           -- Event Time (ì‹¤ì œ ë°ì´í„° ì‹œê°„)
    azimuth DOUBLE PRECISION NOT NULL,
    elevation DOUBLE PRECISION NOT NULL,
    train DOUBLE PRECISION NOT NULL,
    satellite_id BIGINT NOT NULL,
    created_at TIMESTAMP DEFAULT NOW(),     -- Processing Time (ì €ì¥ ì‹œê°, ì°¸ê³ ìš©)

    PRIMARY KEY (id, timestamp)             -- íŒŒí‹°ì…˜ í‚¤ í¬í•¨
) PARTITION BY RANGE (timestamp);

-- ì›”ë³„ íŒŒí‹°ì…˜ ìë™ ìƒì„±
CREATE TABLE tracking_data_2026_01 PARTITION OF tracking_data
    FOR VALUES FROM ('2026-01-01') TO ('2026-02-01');

CREATE TABLE tracking_data_2026_02 PARTITION OF tracking_data
    FOR VALUES FROM ('2026-02-01') TO ('2026-03-01');

-- ì¥ì :
-- 1. ì˜¤ë˜ëœ íŒŒí‹°ì…˜ DROP (DELETEë³´ë‹¤ 100ë°° ë¹ ë¦„)
-- 2. ì¡°íšŒ ì‹œ í•„ìš”í•œ íŒŒí‹°ì…˜ë§Œ ìŠ¤ìº” (ì„±ëŠ¥ í–¥ìƒ)
-- 3. ì¸ë±ìŠ¤ í¬ê¸° ê°ì†Œ
```

### 2.2 ì¸ë±ìŠ¤ ì „ëµ

```sql
-- ë³µí•© ì¸ë±ìŠ¤ (ìì£¼ ì¡°íšŒí•˜ëŠ” íŒ¨í„´)
CREATE INDEX idx_satellite_time
ON tracking_data(satellite_id, timestamp DESC);

-- ì»¤ë²„ë§ ì¸ë±ìŠ¤ (í…Œì´ë¸” ì ‘ê·¼ ë¶ˆí•„ìš”, ê°€ì¥ ë¹ ë¦„)
CREATE INDEX idx_satellite_time_covering
ON tracking_data(satellite_id, timestamp, azimuth, elevation, train);

-- ë¶€ë¶„ ì¸ë±ìŠ¤ (ìµœê·¼ 7ì¼ë§Œ)
CREATE INDEX idx_recent_data
ON tracking_data(timestamp)
WHERE timestamp > NOW() - INTERVAL '7 days';

-- ì„±ëŠ¥ ë¹„êµ:
-- ì¼ë°˜ ì¿¼ë¦¬:     1000ms
-- ë³µí•© ì¸ë±ìŠ¤:   100ms (10ë°° ë¹ ë¦„)
-- ì»¤ë²„ë§ ì¸ë±ìŠ¤: 10ms (100ë°° ë¹ ë¦„)
```

### 2.3 ë°°ì¹˜ INSERT ìµœì í™”

```kotlin
suspend fun batchInsert(dataList: List<TrackingData>): Int {
    // ë‹¨ì¼ ì¿¼ë¦¬ë¡œ 100ê°œ INSERT
    val sql = """
        INSERT INTO tracking_data
        (timestamp, azimuth, elevation, train, satellite_id)
        VALUES ${dataList.joinToString(",") { "(?, ?, ?, ?, ?)" }}
    """.trimIndent()

    return databaseClient.sql(sql)
        .apply {
            dataList.forEachIndexed { i, data ->
                bind(i * 5 + 0, data.timestamp)
                bind(i * 5 + 1, data.azimuth)
                bind(i * 5 + 2, data.elevation)
                bind(i * 5 + 3, data.train)
                bind(i * 5 + 4, data.satelliteId)
            }
        }
        .fetch()
        .rowsUpdated()
        .awaitSingle()
}
```

---

## 3. ë°ì´í„° ë³´ê´€ ì •ì±…

### 3.1 ê³„ì¸µë³„ ë³´ê´€ ì „ëµ

```
ìµœê·¼ 7ì¼:    ì›ë³¸ ë°ì´í„° (100ms ê°„ê²©, ì•½ 6ë°±ë§Œ ê±´)
7-30ì¼:      1ì´ˆ í‰ê·  (100ë°° ì••ì¶•, ì•½ 260ë§Œ ê±´)
30-90ì¼:     1ë¶„ í‰ê·  (6000ë°° ì••ì¶•, ì•½ 8ë§Œ ê±´)
90ì¼ ì´í›„:   ì‚­ì œ ë˜ëŠ” ì•„ì¹´ì´ë¸Œ
```

### 3.2 ìë™ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬

```kotlin
@Service
class DataRetentionService(
    private val repository: TrackingDataRepository,
    private val compressedRepository: CompressedTrackingDataRepository
) {

    // ë§¤ì¼ ìƒˆë²½ 2ì‹œ: 90ì¼ ì´ì „ ë°ì´í„° ì‚­ì œ
    @Scheduled(cron = "0 0 2 * * *")
    fun cleanupOldData() {
        val cutoffDate = Instant.now().minus(90, ChronoUnit.DAYS)

        // íŒŒí‹°ì…˜ DROP (ë¹ ë¦„)
        val partition = "tracking_data_${cutoffDate.atZone(ZoneId.of("UTC")).format(DateTimeFormatter.ofPattern("yyyy_MM"))}"
        repository.dropPartition(partition)

        logger.info("90ì¼ ì´ì „ íŒŒí‹°ì…˜ ì‚­ì œ: $partition")
    }

    // ë§¤ì¼ ìƒˆë²½ 3ì‹œ: 30ì¼ ì´ì „ ë°ì´í„° ì••ì¶•
    @Scheduled(cron = "0 0 3 * * *")
    suspend fun compressOldData() {
        val startDate = Instant.now().minus(30, ChronoUnit.DAYS)
        val endDate = Instant.now().minus(7, ChronoUnit.DAYS)

        val rawData = repository.findByTimestampBetween(startDate, endDate)

        // 1ë¶„ í‰ê· ìœ¼ë¡œ ì••ì¶•
        val compressed = rawData
            .groupBy { it.timestamp.truncatedTo(ChronoUnit.MINUTES) }
            .map { (minute, samples) ->
                CompressedTrackingData(
                    timestamp = minute,
                    avgAzimuth = samples.map { it.azimuth }.average(),
                    avgElevation = samples.map { it.elevation }.average(),
                    avgTrain = samples.map { it.train }.average(),
                    minAzimuth = samples.minOf { it.azimuth },
                    maxAzimuth = samples.maxOf { it.azimuth },
                    sampleCount = samples.size,
                    satelliteId = samples.first().satelliteId
                )
            }

        compressedRepository.batchInsert(compressed)
        repository.deleteByTimestampBetween(startDate, endDate)

        logger.info("ë°ì´í„° ì••ì¶• ì™„ë£Œ: ${compressed.size}ë¶„, ì›ë³¸ ${rawData.size}ê°œ ì‚­ì œ")
    }
}
```

---

## 4. ëª¨ë‹ˆí„°ë§ & ì•ŒëŒ

### 4.1 í—¬ìŠ¤ ì²´í¬

```kotlin
@Service
class DatabaseHealthMonitor(
    private val writer: AsyncDatabaseWriter,
    private val meterRegistry: MeterRegistry
) {

    init {
        // í í¬ê¸° ëª¨ë‹ˆí„°ë§
        Gauge.builder("db.queue.size", writer) { it.getPendingCount().toDouble() }
            .description("DB ì €ì¥ ëŒ€ê¸° í í¬ê¸°")
            .register(meterRegistry)

        // ì €ì¥ ì†ë„ ëª¨ë‹ˆí„°ë§
        Gauge.builder("db.save.rate", writer) { it.getSaveRate().toDouble() }
            .description("ì´ˆë‹¹ DB ì €ì¥ ê°œìˆ˜")
            .register(meterRegistry)
    }

    @Scheduled(fixedRate = 5000)
    fun checkHealth() {
        val queueSize = writer.getPendingCount()
        val saveRate = writer.getSaveRate()

        when {
            queueSize > 5000 -> {
                logger.warn("âš ï¸ DB í 50% ì‚¬ìš© ì¤‘: $queueSize")
                alertService.sendWarning("DB ì €ì¥ ì§€ì—° ê°ì§€",
                    "í: $queueSize, ì €ì¥ì†ë„: $saveRate/s")
            }
            queueSize > 9000 -> {
                logger.error("ğŸ”´ DB í 90% ì‚¬ìš© ì¤‘: $queueSize")
                alertService.sendCritical("DB ì €ì¥ ì‹¬ê° ì§€ì—°",
                    "í: $queueSize, ë°ì´í„° ì†ì‹¤ ìœ„í—˜")
            }
            saveRate == 0L && queueSize > 0 -> {
                logger.error("ğŸ”´ DB ì €ì¥ ì¤‘ë‹¨ ê°ì§€")
                alertService.sendCritical("DB ì—°ê²° ì‹¤íŒ¨ ì¶”ì •")
            }
        }
    }

    @GetMapping("/api/system/db-status")
    fun getStatus(): Map<String, Any> {
        return mapOf(
            "queueSize" to writer.getPendingCount(),
            "queueCapacity" to 10_000,
            "queueUsagePercent" to (writer.getPendingCount() * 100 / 10_000),
            "saveRate" to writer.getSaveRate(),
            "status" to when {
                writer.getPendingCount() < 5000 -> "HEALTHY"
                writer.getPendingCount() < 9000 -> "WARNING"
                else -> "CRITICAL"
            }
        )
    }
}
```

---

## 5. ì„±ëŠ¥ ìµœì í™”

### 5.1 ì—°ê²° í’€ ì„¤ì •

```yaml
spring:
  r2dbc:
    url: r2dbc:postgresql://localhost:5432/acs
    username: acs_user
    password: ${DB_PASSWORD}
    pool:
      initial-size: 10           # ì´ˆê¸° ì»¤ë„¥ì…˜
      max-size: 20              # ìµœëŒ€ ì»¤ë„¥ì…˜
      max-idle-time: 30m        # ìœ íœ´ íƒ€ì„ì•„ì›ƒ
      max-acquire-time: 3s      # ëŒ€ê¸° ì‹œê°„
      validation-query: SELECT 1 # í—¬ìŠ¤ì²´í¬
```

### 5.2 íŠ¸ëœì­ì…˜ ì „ëµ

```kotlin
@Transactional(
    isolation = Isolation.READ_COMMITTED,
    timeout = 5
)
suspend fun batchInsertWithRetry(batch: List<TrackingData>) {
    var retryCount = 0
    val maxRetries = 3

    while (retryCount < maxRetries) {
        try {
            repository.batchInsert(batch)
            return

        } catch (e: DataAccessException) {
            retryCount++
            logger.warn("ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨ (ì¬ì‹œë„ $retryCount/$maxRetries)", e)

            if (retryCount >= maxRetries) {
                // ìµœì¢… ì‹¤íŒ¨: íŒŒì¼ ë°±ì—…
                fallbackWriter.saveToCsv(batch)
                throw e
            }

            delay(1000L * retryCount)  // Exponential backoff
        }
    }
}
```

### 5.3 ì½ê¸° ë³µì œë³¸ (ì„ íƒì )

```
Master DB (ì“°ê¸°)
  â†“ ìŠ¤íŠ¸ë¦¬ë° ë³µì œ
Replica 1 (ì½ê¸°)
Replica 2 (ì½ê¸°)

// ì“°ê¸°
@Transactional
fun save() { ... }  // Masterë¡œ

// ì½ê¸°
@Transactional(readOnly = true)
fun findAll() { ... }  // Replicaë¡œ (ë¶€í•˜ ë¶„ì‚°)
```

---

## 6. ë°±ì—… & ë³µêµ¬

### 6.1 ìë™ ë°±ì—…

```bash
#!/bin/bash
# /scripts/backup.sh

DATE=$(date +%Y%m%d)
BACKUP_DIR=/backup

# ì „ì²´ ë°±ì—… (ë§¤ì¼ ìƒˆë²½ 4ì‹œ)
pg_dump acs_db > $BACKUP_DIR/acs_full_$DATE.sql

# 7ì¼ ì´ì „ ë°±ì—… ì‚­ì œ
find $BACKUP_DIR -name "acs_full_*.sql" -mtime +7 -delete

# S3 ì—…ë¡œë“œ (ì„ íƒ)
aws s3 cp $BACKUP_DIR/acs_full_$DATE.sql s3://acs-backup/
```

```bash
# crontab
0 4 * * * /scripts/backup.sh
```

### 6.2 í¬ì¸íŠ¸-ì¸-íƒ€ì„ ë³µêµ¬

```bash
# WAL ì•„ì¹´ì´ë¹™ í™œì„±í™”
archive_mode = on
archive_command = 'cp %p /archive/%f'

# ë³µêµ¬ ì˜ˆì‹œ: ì˜¤ëŠ˜ 10:30:15ë¡œ ë³µêµ¬
restore_command = 'cp /archive/%f %p'
recovery_target_time = '2026-01-07 10:30:15'
```

---

## 7. í…ŒìŠ¤íŠ¸ ì „ëµ

### 7.1 ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

```kotlin
@Test
fun `ë°°ì¹˜ INSERT ì„±ëŠ¥ í…ŒìŠ¤íŠ¸`() = runBlocking {
    val batch = (1..100).map { createTestData() }

    val startTime = System.currentTimeMillis()
    repository.batchInsert(batch)
    val duration = System.currentTimeMillis() - startTime

    // ëª©í‘œ: 20ms ì´ë‚´
    assertTrue(duration < 20, "ë°°ì¹˜ ì €ì¥ì´ ${duration}ms ì†Œìš” (ëª©í‘œ: 20ms)")
}

@Test
fun `í ë²„í¼ë§ í…ŒìŠ¤íŠ¸`() {
    val writer = AsyncDatabaseWriter(repository)

    // 1000ê°œ ì¶”ê°€
    repeat(1000) {
        writer.add(createTestData())
    }

    // 2ì´ˆ ëŒ€ê¸° (2ë²ˆ í”ŒëŸ¬ì‹œ)
    Thread.sleep(2000)

    // í ë¹„ì—ˆëŠ”ì§€ í™•ì¸
    assertEquals(0, writer.getPendingCount())
}
```

### 7.2 ì¥ì•  ë³µêµ¬ í…ŒìŠ¤íŠ¸

```kotlin
@Test
fun `DB ì¥ì•  ì‹œ í ë²„í¼ë§ í…ŒìŠ¤íŠ¸`() = runBlocking {
    val writer = AsyncDatabaseWriter(mockRepository)

    // DB ì¥ì•  ì‹œë®¬ë ˆì´ì…˜
    whenever(mockRepository.batchInsert(any())).thenThrow(DataAccessException::class.java)

    // 100ê°œ ì¶”ê°€
    repeat(100) { writer.add(createTestData()) }

    // íì— ë‚¨ì•„ìˆì–´ì•¼ í•¨
    assertEquals(100, writer.getPendingCount())

    // DB ë³µêµ¬
    whenever(mockRepository.batchInsert(any())).thenReturn(100)

    // 1ì´ˆ í›„ ì €ì¥ ì™„ë£Œ
    delay(1500)
    assertEquals(0, writer.getPendingCount())
}
```

---

## 8. êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 2.5: ë°ì´í„° ê³„ì¸µ êµ¬ì¶• (2-3ì¼)

#### Day 1: ê¸°ë³¸ êµ¬ì¡°
- [ ] AsyncDatabaseWriter êµ¬í˜„
- [ ] TrackingDataRepository (R2DBC)
- [ ] ë°°ì¹˜ INSERT ì¿¼ë¦¬
- [ ] í…Œì´ë¸” ìƒì„± (íŒŒí‹°ì…”ë‹)
- [ ] ì¸ë±ìŠ¤ ìƒì„±

#### Day 2: ëª¨ë‹ˆí„°ë§ & ìµœì í™”
- [ ] DatabaseHealthMonitor êµ¬í˜„
- [ ] Metrics ì„¤ì • (Micrometer)
- [ ] ì•ŒëŒ ì„¤ì • (í ì„ê³„ê°’)
- [ ] íŠ¸ëœì­ì…˜ ì¬ì‹œë„ ë¡œì§
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

#### Day 3: ë³´ê´€ ì •ì±… & ë°±ì—…
- [ ] DataRetentionService êµ¬í˜„
- [ ] ì••ì¶• í…Œì´ë¸” ìƒì„±
- [ ] ìë™ ë°±ì—… ìŠ¤í¬ë¦½íŠ¸
- [ ] Flyway ë§ˆì´ê·¸ë ˆì´ì…˜
- [ ] í†µí•© í…ŒìŠ¤íŠ¸

---

## 9. ìš´ì˜ ê°€ì´ë“œ

### 9.1 ëª¨ë‹ˆí„°ë§ ì§€í‘œ

| ì§€í‘œ | ì •ìƒ | ê²½ê³  | ìœ„í—˜ |
|------|------|------|------|
| **í í¬ê¸°** | <5,000 | 5,000-9,000 | >9,000 |
| **ì €ì¥ ì†ë„** | ~100/s | <50/s | 0/s |
| **ë°°ì¹˜ ì§€ì—°** | <20ms | 20-50ms | >50ms |
| **DB CPU** | <30% | 30-70% | >70% |

### 9.2 ì¥ì•  ëŒ€ì‘

```
ì‹œë‚˜ë¦¬ì˜¤ 1: í 50% ì´ˆê³¼
â†’ ì¡°ì¹˜: DB ì—°ê²° í™•ì¸, ìŠ¬ë¡œìš° ì¿¼ë¦¬ ë¶„ì„

ì‹œë‚˜ë¦¬ì˜¤ 2: í 90% ì´ˆê³¼
â†’ ì¡°ì¹˜: ê¸´ê¸‰ - ë°°ì¹˜ í¬ê¸° ì¦ê°€, í”ŒëŸ¬ì‹œ ê°„ê²© ë‹¨ì¶•

ì‹œë‚˜ë¦¬ì˜¤ 3: ì €ì¥ ì†ë„ 0
â†’ ì¡°ì¹˜: DB ì¬ì‹œì‘, í ë°ì´í„° CSV ë°±ì—…

ì‹œë‚˜ë¦¬ì˜¤ 4: ë””ìŠ¤í¬ ìš©ëŸ‰ 80% ì´ˆê³¼
â†’ ì¡°ì¹˜: ì˜¤ë˜ëœ íŒŒí‹°ì…˜ ìˆ˜ë™ ì‚­ì œ
```

### 9.3 ì„¤ì • íŒŒì¼

```yaml
# application.yml
acs:
  database:
    # ë°°ì¹˜ ì„¤ì •
    batch:
      size: 100                    # ë°°ì¹˜ í¬ê¸°
      flush-interval-ms: 1000      # í”ŒëŸ¬ì‹œ ê°„ê²©
      queue-capacity: 10000        # í í¬ê¸°

    # ë³´ê´€ ì •ì±…
    retention:
      raw-data-days: 7             # ì›ë³¸ ë°ì´í„° ë³´ê´€
      compressed-data-days: 90     # ì••ì¶• ë°ì´í„° ë³´ê´€
      enable-compression: true     # ì••ì¶• í™œì„±í™”

    # ëª¨ë‹ˆí„°ë§
    monitoring:
      queue-warning-threshold: 5000
      queue-critical-threshold: 9000
      alert-enabled: true
      metrics-enabled: true
```

---

## 10. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### 10.1 ì‹¤ì¸¡ ë°ì´í„°

| ì‘ì—… | ì†Œìš” ì‹œê°„ | ì²˜ë¦¬ëŸ‰ |
|------|----------|--------|
| **í ì¶”ê°€** | <1Î¼s | 1,000,000/s |
| **ë°°ì¹˜ INSERT (100ê°œ)** | 15-20ms | 5,000ê°œ/s |
| **ë‹¨ì¼ INSERT** | 5-10ms | 100ê°œ/s |
| **ì¡°íšŒ (ì¸ë±ìŠ¤)** | 10ms | 10,000ê°œ/s |
| **ì¡°íšŒ (í’€ìŠ¤ìº”)** | 1000ms | 1,000ê°œ/s |

### 10.2 ì‹œìŠ¤í…œ ë¶€í•˜

```
í˜„ì¬ ë¶€í•˜: ì´ˆë‹¹ 100ê°œ
ì²˜ë¦¬ ê°€ëŠ¥: ì´ˆë‹¹ 5,000ê°œ
ì—¬ìœ : 50ë°°

ë°±ì—”ë“œ CPU: 2% (ì €ì¥ ì‘ì—…)
ë°±ì—”ë“œ ë©”ëª¨ë¦¬: 5 MB (í ë²„í¼)
í”„ë¡ íŠ¸ì—”ë“œ ì˜í–¥: 0% (ì™„ì „ ë¶„ë¦¬)
```

---

## 11. FAQ

**Q1: í”„ë¡ íŠ¸ì—”ë“œì— ì˜í–¥ ìˆë‚˜ìš”?**
A: ì „í˜€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡ íŠ¸ì—”ë“œëŠ” ë©”ëª¨ë¦¬ì˜ ë°ì´í„°ë§Œ ë³´ë©°, DB ì €ì¥ì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë…ë¦½ì ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.

**Q2: DB ì¥ì•  ì‹œ ë°ì´í„° ì†ì‹¤ë˜ë‚˜ìš”?**
A: ì•„ë‹™ë‹ˆë‹¤. íì— ìµœëŒ€ 100ì´ˆ ë¶„ëŸ‰(10,000ê°œ) ë²„í¼ë§ë˜ë©°, ë³µêµ¬ ì‹œ ëª¨ë‘ ì €ì¥ë©ë‹ˆë‹¤.

**Q3: ì €ì¥ ì‹œê°„ì´ ëŠ¦ì–´ì§€ë©´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?**
A: timestampëŠ” ì‹¤ì œ ë°ì´í„° ì‹œê°„ì´ë¯€ë¡œ, ëŠ¦ê²Œ ì €ì¥ë˜ì–´ë„ ì‹œê°„ ì •ë³´ëŠ” ì •í™•íˆ ë³´ì¡´ë©ë‹ˆë‹¤.

**Q4: ì–¼ë§ˆë‚˜ ë¹ ë¥¸ê°€ìš”?**
A: ê°œë³„ ì €ì¥(1000ms) ëŒ€ë¹„ ë°°ì¹˜ ì €ì¥(20ms)ì€ 50ë°° ë¹ ë¦…ë‹ˆë‹¤. ì‹¤ì‹œê°„ ì„±ëŠ¥ì— ì˜í–¥ ì—†ìŠµë‹ˆë‹¤.

**Q5: ë°ì´í„°ëŠ” ì–¼ë§ˆë‚˜ ë³´ê´€í•˜ë‚˜ìš”?**
A: ì›ë³¸ 7ì¼, ì••ì¶• 90ì¼, ì´í›„ ì‚­ì œì…ë‹ˆë‹¤. ì •ì±…ì€ ì„¤ì •ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥í•©ë‹ˆë‹¤.

---

## 12. ì°¸ê³  ìë£Œ

### ê´€ë ¨ ë¬¸ì„œ
- [Backend_Refactoring_plan.md](./Backend_Refactoring_plan.md) - 13ì¥: ì‹¤ì‹œê°„ DB ì €ì¥ ì „ëµ
- [Expert_Analysis_Report.md](./Expert_Analysis_Report.md) - P1: ë°ì´í„° ê³„ì¸µ ìµœì í™”
- [SYSTEM_OVERVIEW.md](../../references/architecture/SYSTEM_OVERVIEW.md) - ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì™¸ë¶€ ì°¸ì¡°
- Spring Data R2DBC: https://spring.io/projects/spring-data-r2dbc
- PostgreSQL Partitioning: https://www.postgresql.org/docs/current/ddl-partitioning.html
- R2DBC Driver: https://r2dbc.io/

---

**ë¬¸ì„œ ë²„ì „**: 1.0.0
**ìµœì¢… ìˆ˜ì •**: 2026-01-07
**ì‘ì„±ì**: ACS Architecture Team
**ê²€í† ì**: (Pending)
